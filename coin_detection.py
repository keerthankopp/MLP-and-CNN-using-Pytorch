# -*- coding: utf-8 -*-
"""Coin Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRmeTsQSwgptJdxOuqrJPL1XfXXi0oZv
"""

!nvcc --version

!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git

# Commented out IPython magic to ensure Python compatibility.
# %load_ext nvcc_plugin

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
import os
import sklearn.model_selection
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset

import torch, torchvision
from torch import nn,optim
from torch.autograd import Variable



import torch.nn.functional as F
from matplotlib import pyplot as plt



from google.colab import drive
drive.mount("/content/gdrive")

"""Hyperparameters"""

BATCH_SIZE = 16; 
EPOCHS = 30; 
LEARNING_RATE = 0.002; 
RANDOM_SEED = 1

MAX_LEN = 769024
STD_DEV = 25554.741849333524
# GPU Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""Preprocessing functions

"""

def sort_files():

    file_path = "/content/gdrive/My Drive/data_raw/"#go to path of files
    
    '''
    can you check how to implement the next 2 lines here because we arent using the "os" library
    '''
    file_names = os.listdir(file_path)#get file names from path
    file_names.sort()#sort names
    
    file_list = []
    for name in file_names:
        file_list.append(file_path+name)#append file path to file names


    
    return file_list#return list of files

class AudioDataset(Dataset):#dataset class to hold data for loading into model
    def __init__(self, transform=None):
        
        self.scaler = StandardScaler()#normalizing function
        self.max_len = 769024
        self.paths = sort_files()#store paths of all data instead of storing actual data
        print(self.paths[1])
        
    def __getitem__(self, index):#output data & label given input index
        
        d = np.loadtxt(self.paths[index],delimiter=',', dtype=float)#get data from files to numpy array
        d = np.pad(d, (0, self.max_len - len(d)), 'constant', constant_values=0)#zero padding for uneven time samples
        
        d = d.reshape(-1,1)
        d = d/STD_DEV #normalize, div by stddev is enough
        
        
        if self.paths[index].startswith('/content/gdrive/My Drive/data_raw/200'):#add labels
            y = np.array([0, 0, 0 ,0, 0, 0, 1])
        elif self.paths[index].startswith('/content/gdrive/My Drive/data_raw/100'):
            y = np.array([0, 0, 0, 0, 0 ,1, 0])
        elif self.paths[index].startswith('/content/gdrive/My Drive/data_raw/50'):
            y = np.array([0, 0, 0, 0, 1, 0 ,0])
        elif self.paths[index].startswith('/content/gdrive/My Drive/data_raw/20'):
            y = np.array([0 ,0 ,0 ,1, 0 ,0 ,0])
        elif self.paths[index].startswith('/content/gdrive/My Drive/data_raw/5'):
            y = np.array([0, 0, 1, 0, 0, 0, 0])
        elif self.paths[index].startswith('/content/gdrive/My Drive/data_raw/2'):
            y = np.array([0, 1 ,0 ,0, 0, 0 ,0])
        elif self.paths[index].startswith('/content/gdrive/My Drive/data_raw/1'):
            y = np.array([1, 0, 0,0 ,0 ,0, 0])
        
        
        return torch.tensor(d).float(), torch.LongTensor(y).float()
    
    def __len__(self):#output lengths of dataset
        return len(self.paths)

def get_split_data():

    full_data = AudioDataset()#create dataset object
    
    
    proportions = [.75, .10, .15]
    lengths = [int(p * len(full_data)) for p in proportions]
    lengths[-1] = len(full_data) - sum(lengths[:-1])
    train_data,val_data, test_data = torch.utils.data.random_split(full_data, lengths)#split into train,val, test dataset

    return train_data, test_data, val_data

"""models

"""

class MLPModel(nn.Module):# MLP network 769024 x 751 x 28 x 7
    def __init__(self,input_size, num_classes):#initialize layers of the model
        super(MLPModel, self).__init__()
               
        self.fc1 = nn.Sequential(
            nn.Linear(input_size, int(input_size/2**10), bias=True),
            nn.BatchNorm1d(int(input_size/2**10)),
            nn.Sigmoid())
        self.fc3 = nn.Sequential(
            nn.Linear(int(input_size/2**10), int(num_classes*4), bias=True),
            nn.BatchNorm1d(int(num_classes*4)),
            nn.Sigmoid())
        self.fc4 = nn.Sequential(
            nn.Linear(int(num_classes*4), num_classes, bias=True),
            nn.BatchNorm1d(num_classes),
            nn.Sigmoid())
            

    def forward(self, x):#forward path for model
        x = x.view(x.size(0), -1) 
        x = self.fc1(x)
        x = self.fc3(x)
        x = self.fc4(x)
        return x

class CNNModel(nn.Module):# CNN 6x (1x3 conv -> batch norm -> ReLU -> maxpool 2) -> 100 x 7 FC layer

    def __init__(self,input_size, filter_size, kernel_size, num_channels, num_classes):#initialize layers of model
        super(CNNModel, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv1d(num_channels ,filter_size, kernel_size=kernel_size, stride=1, padding=1),
            nn.BatchNorm1d(filter_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        
        self.layer2 = nn.Sequential(
            nn.Conv1d(filter_size ,filter_size, kernel_size=kernel_size, stride=1, padding=1),
            nn.BatchNorm1d(filter_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        
        self.layer3 = nn.Sequential(
            nn.Conv1d(filter_size ,filter_size, kernel_size=kernel_size, stride=1, padding=1),
            nn.BatchNorm1d(filter_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        
        self.layer4 = nn.Sequential(
            nn.Conv1d(filter_size ,filter_size, kernel_size=kernel_size, stride=1, padding=1),
            nn.BatchNorm1d(filter_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        
        self.layer5 = nn.Sequential(
            nn.Conv1d(filter_size ,filter_size, kernel_size=kernel_size, stride=1, padding=1),
            nn.BatchNorm1d(filter_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
        
        self.layer6 = nn.Sequential(
            nn.Conv1d(filter_size ,filter_size, kernel_size=kernel_size, stride=1, padding=1),
            nn.BatchNorm1d(filter_size),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2))
                
        self.fc1 = nn.Sequential(
            nn.Linear(int(input_size/(2**6))*filter_size, 100, bias=True),
            nn.BatchNorm1d(100),
            nn.Sigmoid())
        self.fc2 = nn.Sequential(
            nn.Linear(100, num_classes, bias=True),
            nn.BatchNorm1d(num_classes),
            nn.Sigmoid())
            

    def forward(self, x):#forward path of model
        x = x.transpose(1, 2).contiguous()#reshape data for compatibility with torch conv layers
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.layer6(x)
        x = x.view(x.size(0), -1)#flatten
        tsnedata.append(x)  #pass data for tsne later 
        x = self.fc1(x)
        x = self.fc2(x)
        return x

"""running script"""

def validate(val_batch):
    model.eval();  
    with torch.no_grad():
        X, y = val_batch;
        X, y = X.to(device), y.to(device);
        out = model(X); 

        predictions = torch.argmax(out, dim=1); 
        samples_correct = predictions[predictions == torch.argmax(y,dim=1)].shape[0]; 

        val_loss = criterion(out, y); ' criterion = nn.CrossEntropyLoss();  '
        val_accuracy = samples_correct/float(X.shape[0]) * 100; 
        
    model.train(); 
    return val_loss, val_accuracy;

"""=======================================LOAD DATA======================== run this first to keep data loaded"""

train_data, test_data, val_data = get_split_data()#load datasets
print("data loaded")

"""================================================ create model======================================"""

train_loader = torch.utils.data.DataLoader(train_data, BATCH_SIZE);
val_loader = torch.utils.data.DataLoader(val_data, BATCH_SIZE); 
test_loader = torch.utils.data.DataLoader(test_data, BATCH_SIZE);#data loaders
tsnedata = []

torch.manual_seed(RANDOM_SEED)

'''
call model
'''
#model = TESTModel(input_size=769024, filter_size=16, kernel_size=3, num_channels=1, num_classes=7, hidden_size=28)
model = CNNModel(input_size=769024, filter_size=16, kernel_size=3, num_channels=1, num_classes=7)
#model = MLPModel(num_classes=7,input_size = 769024)

model = model.to(device)#model to GPU
criterion = nn.CrossEntropyLoss(); 
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE); 

print(model)

"""===============================================train============================================"""

train_losses = [];
train_accuracies = []; 
val_losses = []; 
val_accuracies = []; 

val_iterator = iter(val_loader); 
print(device)
for epoch in range(EPOCHS): 
    print(epoch)
    model.train()
    for i, (X, y) in enumerate(train_loader):
        #input (769024,batch_size,1)
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad(); 
        out = model(X); 
        #output (7,batch_size, 1)

        predictions = torch.argmax(out, dim=1);#convert 1hot encoding to numbers
        batch_samples_correct = predictions[predictions == torch.argmax(y,dim=1)].shape[0];
        batch_accuracy = batch_samples_correct / float(y.shape[0]) * 100; 
        batch_loss = criterion(out, y); 
        batch_loss.backward(); #backpropagation
        optimizer.step();#update weights
        print(batch_accuracy)
        '''
        try:
            val_loss, val_accuracy = validate(next(val_iterator));
        except StopIteration or UnicodeDecodeError:
            val_iterator = iter(val_loader);
            val_loss, val_accuracy = validate(next(val_iterator));
        '''
        train_losses.append(batch_loss.item());
        train_accuracies.append(batch_accuracy);
        '''
        val_losses.append(val_loss.item());
        val_accuracies.append(val_accuracy);
        '''

"""========================================evaluate====================================="""

#load model from pickle file
model.load_state_dict(torch.load("/content/gdrive/My Drive/data_raw/cnn.pkl"))
model.eval();
tsnedata = []
tsney = []

with torch.no_grad(): 
    correct = 0; 
    total = 0; 
    for X, y in test_loader: 
        X, y = X.to(device), y.to(device);
        out = model(X); 

        predictions = torch.argmax(out, dim=1); 
        tsney.append(y)
        correct += predictions[predictions == torch.argmax(y,dim=1)].shape[0]; 
        total += float(y.shape[0]); 
    print(f"TEST ACCURACY: {correct / total * 100}%");

"""Now perform t-SNE"""

#prepare inputs and classes for t-SNE
d = torch.cat(tsnedata)
d.shape
y = torch.argmax(torch.cat(tsney),dim=1)
y.shape

import numpy as np
from matplotlib import offsetbox
from sklearn.preprocessing import MinMaxScaler

digits = ["1ct","2ct","5ct","20ct","50ct","1eur","2eur"]#classes
plt.rcParams['figure.dpi'] = 300
def plot_embedding(X, title):#function to plot t-SNE embedding
    _, ax = plt.subplots()
    X = MinMaxScaler().fit_transform(X)

    for digit in range(7):
        ax.scatter(
            *X[y.cpu() == digit].T,
            marker=f"${digits[digit]}$",
            s=60,
            color=plt.cm.Dark2(digit),
            alpha=0.425,
            zorder=2,
        )
    shown_images = np.array([[1.0, 1.0]])  # just something big
    for i in range(X.shape[0]):
        # plot every class on the embedding
        dist = np.sum((X[i] - shown_images) ** 2, 1)
        if np.min(dist) < 4e-3:
            # don't show points that are too close
            continue
        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)
        

    ax.set_title(title)
    ax.axis("off")

from sklearn.manifold import (
    TSNE,
)
from sklearn.pipeline import make_pipeline

embeddings = {
    
    "t-SNE embedding": TSNE(
        n_components=2,
        n_iter=500,
        n_iter_without_progress=150,
        n_jobs=2,
        random_state=0,
    ),
    
}

from time import time

projections, timing = {}, {}
for name, transformer in embeddings.items():
    if name.startswith("Linear Discriminant Analysis"):
        data = d.cpu()
        data.flat[:: d.shape[1] + 1] += 0.01  # Make X invertible
    else:
        data = d.cpu()

    print(f"Computing {name}...")
    start_time = time()
    projections[name] = transformer.fit_transform(data.cpu(), y.cpu())
    timing[name] = time() - start_time

for name in timing:
    title = f"{name} (time {timing[name]:.3f}s)"
    plot_embedding(projections[name], title)

plt.show()
plt.savefig("/content/gdrive/My Drive/data_raw/tsne.jpg",dpi = 1200)